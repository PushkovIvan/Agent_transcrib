{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6576f0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ba7442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Привет!' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 12, 'completion_tokens': 4, 'total_tokens': 16, 'precached_prompt_tokens': 2}, 'model_name': 'GigaChat-2-Max:2.0.28.2', 'x_headers': {'x-request-id': '5d981e7c-bbbe-4938-8aee-183d2fec4924', 'x-session-id': '9a0683d1-f042-4d28-82b1-eb1a242009f9', 'x-client-id': None}, 'finish_reason': 'stop'} id='5d981e7c-bbbe-4938-8aee-183d2fec4924' usage_metadata={'output_tokens': 4, 'input_tokens': 12, 'total_tokens': 16, 'input_token_details': {'cache_read': 2}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_gigachat.chat_models import GigaChat\n",
    "\n",
    "from config import CONFIG\n",
    "\n",
    "\n",
    "giga = GigaChat(\n",
    "    # auth_url = \"https://sm-auth-sd.prom-88-89-apps.ocp-geo.ocp.sigma.sbrf.ru/api/v2/oauth\",\n",
    "    scope = 'GIGACHAT_API_CORP',\n",
    "    # Для авторизации запросов используйте ключ, полученный в проекте GigaChat API\n",
    "    credentials=CONFIG[\"token\"][\"gigachat\"],\n",
    "    verify_ssl_certs=False,\n",
    "    model = \"GigaChat-2-Max\"\n",
    ")\n",
    "messages = \"privet\"\n",
    "res = giga.invoke(messages)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3e134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivan/develop/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import whisper\n",
    "import os\n",
    "import torch\n",
    "import traceback\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from st_audiorec import st_audiorec\n",
    "from langchain_gigachat.chat_models import GigaChat\n",
    "from transformers import Wav2Vec2FeatureExtractor, UniSpeechSatForAudioFrameClassification\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from config import CONFIG\n",
    "from tqdm import tqdm\n",
    "\n",
    "giga = GigaChat(\n",
    "    scope='GIGACHAT_API_CORP',\n",
    "    credentials=CONFIG[\"token\"][\"gigachat\"],\n",
    "    verify_ssl_certs=False,\n",
    "    model=\"GigaChat-2-Max\"\n",
    ")\n",
    "\n",
    "torch.classes.__path__ = []\n",
    "\n",
    "def generate_detailed_analysis_prompt(transcription_text):\n",
    "    prompt = f\"\"\"  \n",
    "Обработай транскрипт рабочего совещания. Сначала попробуй определить участников по контексту (диаризация по наитию), группируя реплики по условным ролям или именам (например: \"Иван\", \"Участник 1\", \"Менеджер проекта\"). Затем выдели: 1) Основные темы/проблемы (кратко суть), 2) Поручения (что, кому/роли, сроки если есть), 3) Ключевые решения, 4) Важные детали (цифры, риски, контакты). Не используй markdown. Если списки - только текстовые обозначения типа \"а)\" или \"-\". Группируй информацию по смыслу. Структура ответа: сначала условные говорящие, затем остальные категории. Если диаризация невозможна - пропусти её и выдели только суть.\n",
    "\n",
    "Транскрипт:\n",
    "{transcription_text}  \n",
    "\n",
    "\"\"\" \n",
    "    return prompt\n",
    "\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "    whisper_model = whisper.load_model(\"large-v3\")\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"microsoft/unispeech-sat-base-plus\")\n",
    "    embedding_model = UniSpeechSatForAudioFrameClassification.from_pretrained(\"microsoft/unispeech-sat-base-plus\")\n",
    "    \n",
    "    return whisper_model, feature_extractor, embedding_model\n",
    "\n",
    "def analyze_transcription(text):\n",
    "    try:\n",
    "        prompt = generate_detailed_analysis_prompt(text)\n",
    "        analysis_result = giga.invoke(prompt)\n",
    "        return analysis_result.content\n",
    "    except Exception as e:\n",
    "        st.error(f\"Ошибка при анализе текста: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def save_audio(audio_bytes, filename):\n",
    "    try:\n",
    "        if not audio_bytes or len(audio_bytes) < 1024:\n",
    "            st.warning(\"Аудиоданные слишком короткие или отсутствуют\")\n",
    "            return None\n",
    "            \n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(audio_bytes)\n",
    "        \n",
    "        if os.path.getsize(filename) < 1024:\n",
    "            st.warning(\"Созданный аудиофайл слишком мал\")\n",
    "            return None\n",
    "            \n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        st.error(f\"Ошибка при сохранении аудио: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "def cleanup_files(*files):\n",
    "    for file in files:\n",
    "        try:\n",
    "            if file and os.path.exists(file):\n",
    "                os.remove(file)\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Не удалось удалить файл {file}: {str(e)}\")\n",
    "\n",
    "def extract_embeddings(audio_path, feature_extractor, model, sr=16000):\n",
    "    \"\"\"Извлечение эмбеддингов с помощью UniSpeechSat\"\"\"\n",
    "    try:\n",
    "        waveform, orig_sr = torchaudio.load(audio_path)\n",
    "        \n",
    "        if orig_sr != sr:\n",
    "            resampler = torchaudio.transforms.Resample(orig_sr, sr)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        inputs = feature_extractor(\n",
    "            waveform.squeeze().numpy(),\n",
    "            sampling_rate=sr,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            embeddings = outputs.hidden_states[-1].mean(dim=1).squeeze().numpy()\n",
    "        \n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        st.error(f\"Ошибка при извлечении эмбеддингов: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def segment_audio(audio_path, segment_length=3, overlap=1, sr=16000):\n",
    "    \"\"\"Сегментация аудио на фрагменты\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=sr)\n",
    "        duration = len(y) / sr\n",
    "        segments = []\n",
    "        \n",
    "        start = 0.0\n",
    "        while start + segment_length <= duration:\n",
    "            end = start + segment_length\n",
    "            segment = y[int(start*sr):int(end*sr)]\n",
    "            segments.append((start, end, segment))\n",
    "            start += segment_length - overlap\n",
    "        \n",
    "        return segments\n",
    "    except Exception as e:\n",
    "        st.error(f\"Ошибка при сегментации аудио: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def cluster_speakers(embeddings, n_speakers=None):\n",
    "    \"\"\"Кластеризация говорящих\"\"\"\n",
    "    try:\n",
    "        if n_speakers is None:\n",
    "            n_speakers = min(5, max(2, len(embeddings) // 3))\n",
    "        \n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=n_speakers,\n",
    "            metric='cosine',\n",
    "            linkage='average'\n",
    "        ).fit(embeddings)\n",
    "        \n",
    "        return clustering.labels_\n",
    "    except Exception as e:\n",
    "        st.error(f\"Ошибка при кластеризации: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def transcribe_with_speakers(whisper_model, audio_path, segments, labels):\n",
    "    \"\"\"Транскрипция с определением говорящих\"\"\"\n",
    "    try:\n",
    "        result = whisper_model.transcribe(audio_path, language=\"ru\")\n",
    "        tagged_segments = []\n",
    "        for segment in tqdm(result[\"segments\"]):\n",
    "            seg_start = segment[\"start\"]\n",
    "            seg_end = segment[\"end\"]\n",
    "            speaker = \"Unknown\"\n",
    "            for (start, end, _), label in zip(segments, labels):\n",
    "                if not (seg_end <= start or seg_start >= end):\n",
    "                    speaker = f\"Speaker {label+1}\"\n",
    "                    break\n",
    "            \n",
    "            tagged_segments.append({\n",
    "                \"start\": seg_start,\n",
    "                \"end\": seg_end,\n",
    "                \"text\": segment[\"text\"],\n",
    "                \"speaker\": speaker\n",
    "            })\n",
    "        \n",
    "        return tagged_segments\n",
    "    except Exception as e:\n",
    "        st.error(f\"Ошибка при транскрипции: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def format_transcription(tagged_segments):\n",
    "    \"\"\"Форматирование результата транскрипции\"\"\"\n",
    "    formatted = []\n",
    "    current_speaker = None\n",
    "    \n",
    "    for seg in tqdm(tagged_segments):\n",
    "        if seg[\"speaker\"] != current_speaker:\n",
    "            formatted.append(f\"\\n\\n**{seg['speaker']}:**\\n\")\n",
    "            current_speaker = seg[\"speaker\"]\n",
    "        formatted.append(seg[\"text\"] + \" \")\n",
    "    \n",
    "    return \"\".join(formatted).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cffdca7",
   "metadata": {},
   "source": [
    "### Прогон для записи демо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c87a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"output.wav\"\n",
    "whisper_model, feature_extractor, embedding_model = load_models() \n",
    "segments = segment_audio(audio_path)\n",
    "\n",
    "if segments:\n",
    "    embeddings = []\n",
    "    for start, end, segment in tqdm(segments):\n",
    "        segment_path = f\"temp_segment_{start:.1f}_{end:.1f}.wav\"\n",
    "        sf.write(segment_path, segment, 48000, 'PCM_24')\n",
    "        \n",
    "        emb = extract_embeddings(segment_path, feature_extractor, embedding_model)\n",
    "        if emb is not None:\n",
    "            embeddings.append(emb)\n",
    "        \n",
    "        cleanup_files(segment_path)\n",
    "    \n",
    "    if embeddings:\n",
    "        print(\"кластеризуем спикеров\")\n",
    "        labels = cluster_speakers(np.array(embeddings))\n",
    "        \n",
    "        if labels is not None:\n",
    "            print(\"транскрибируем со спикерами\")\n",
    "            tagged_segments = transcribe_with_speakers(\n",
    "                whisper_model, audio_path, segments, labels\n",
    "            ) \n",
    "            \n",
    "            if tagged_segments:\n",
    "                print(\"Формирую транскрибацию\")\n",
    "                transcription = format_transcription(tagged_segments)\n",
    "                print(\"Вызываю Гигачат\")\n",
    "                analysis = analyze_transcription(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b0d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55205cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = analyze_transcription(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ba37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Условные роли:\n",
      "- Speaker 1 — Менеджер проекта / руководитель направления\n",
      "- Speaker 2 — Специалист аналитического отдела / эксперт по проектам автоматизации\n",
      "- Speaker 4 — Сотрудник технического подразделения\n",
      "- Speaker 5 — Участник обсуждения\n",
      "\n",
      "Основные темы/проблемы:\n",
      "- Оптимизация процесса обработки заявок на лизинг легковых автомобилей\n",
      "- Повышение эффективности коммуникации с клиентом путем внедрения автоматизированных инструментов\n",
      "- Создание быстрого и удобного клиентского пути для оформления лизинга\n",
      "- Необходимость разработки новой модели взаимодействия с клиентами через сайт Сберлизинга\n",
      "- Интеграция технологий искусственного интеллекта и чат-ботов для улучшения обслуживания клиентов\n",
      "\n",
      "Поручения:\n",
      "- Разработать DS-модель для определения целевых клиентов, готовых взять лизинг легковой техники (готовность к середине июня)\n",
      "- Подготовить массовую рассылку уведомлениям клиентам о предложениях Сбербанка по лизингу (до конца июня)\n",
      "- Реализовать механизм расчета стоимости лизинга на сайте Сберлизинга (реализация до середины июля)\n",
      "- Проверить функциональность текущего лендинга на сайте Сберлизинга для обеспечения полного цикла получения коммерческого предложения (до конца июля)\n",
      "- Проанализировать необходимость интеграции агента-чата на сайте Сберлизинга для консультаций по вопросам лизинга (решение до конца третьего квартала)\n",
      "\n",
      "Ключевые решения:\n",
      "- Для повышения качества коммуникаций отказаться от телефонных обзвонок и использовать персонализированные электронные письма с привлечением лендинга и калькулятора на сайте Сберлизинга\n",
      "- Автоматическое направление запросов клиентов специалистам Сберлизинга с последующей обработкой индивидуальных заявок\n",
      "- Использование DS-моделей для выявления наиболее перспективных клиентов\n",
      "- Постепенная интеграция новых решений в процессы банковского сектора с последующим переносом всего функционала в СбербанкБизнес\n",
      "\n",
      "Важные детали:\n",
      "- Цель достигнуть эффективности работы процессов подачи заявок на лизинг в ближайшем будущем – ориентировочно конец второго полугодия 2025 г.\n",
      "- Запуск DS-модели на практике запланирован на июнь–июль 2023 г., охватывая Москву.\n",
      "- Планируется постепенное внедрение агентизированных сервисов для уменьшения нагрузки на специалистов банка и улучшение качества услуг.\n"
     ]
    }
   ],
   "source": [
    "print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d3af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcribe_03_06_full.txt\", \"w\") as file:\n",
    "    file.write(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc882628",
   "metadata": {},
   "source": [
    "### Пайплайн для полиции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb018458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "\n",
    "def download_youtube_audio(url, output_dir=\"audio\"):\n",
    "    \"\"\"Скачивает аудио из YouTube видео и сохраняет в указанную директорию\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Извлекаем ID видео из URL\n",
    "    if \"youtube.com/watch\" in url:\n",
    "        video_id = url.split(\"v=\")[1].split(\"&\")[0]\n",
    "    else:\n",
    "        video_id = url.split(\"/\")[-1]\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"{video_id}.m4a\")\n",
    "    \n",
    "    # Команда для youtube-dl\n",
    "    command = [\n",
    "        \"yt-dlp\",\n",
    "        \"-x\",  # Извлекаем только аудио\n",
    "        \"--audio-format\", \"m4a\",  # Формат аудио\n",
    "        \"-o\", output_path,  # Путь для сохранения\n",
    "        url\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        return output_path\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Ошибка при загрузке видео {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_audio(audio_path):\n",
    "    \"\"\"Обрабатывает аудиофайл и возвращает транскрипцию и анализ\"\"\"\n",
    "    whisper_model, feature_extractor, embedding_model = load_models() \n",
    "    segments = segment_audio(audio_path)\n",
    "    \n",
    "    if not segments:\n",
    "        return None, None\n",
    "    \n",
    "    embeddings = []\n",
    "    for start, end, segment in tqdm(segments):\n",
    "        segment_path = f\"temp_segment_{start:.1f}_{end:.1f}.wav\"\n",
    "        sf.write(segment_path, segment, 48000, 'PCM_24')\n",
    "        \n",
    "        emb = extract_embeddings(segment_path, feature_extractor, embedding_model)\n",
    "        if emb is not None:\n",
    "            embeddings.append(emb)\n",
    "        \n",
    "        cleanup_files(segment_path)\n",
    "    \n",
    "    if not embeddings:\n",
    "        return None, None\n",
    "    \n",
    "    print(\"Кластеризуем спикеров\")\n",
    "    labels = cluster_speakers(np.array(embeddings))\n",
    "    \n",
    "    if labels is None:\n",
    "        return None, None\n",
    "    \n",
    "    print(\"Транскрибируем со спикерами\")\n",
    "    tagged_segments = transcribe_with_speakers(\n",
    "        whisper_model, audio_path, segments, labels\n",
    "    ) \n",
    "    \n",
    "    if not tagged_segments:\n",
    "        return None, None\n",
    "    \n",
    "    print(\"Формирую транскрибацию\")\n",
    "    transcription = format_transcription(tagged_segments)\n",
    "    # print(\"Вызываю Гигачат\")\n",
    "    # analysis = analyze_transcription(transcription)\n",
    "    analysis = \"\"\n",
    "    \n",
    "    return transcription, analysis\n",
    "\n",
    "def save_to_markdown(video_url, transcription, analysis, output_dir=\"transcriptions\"):\n",
    "    \"\"\"Сохраняет результаты в Markdown файл\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Создаем имя файла из ID видео\n",
    "    if \"youtube.com/watch\" in video_url:\n",
    "        video_id = video_url.split(\"v=\")[1].split(\"&\")[0]\n",
    "    else:\n",
    "        video_id = video_url.split(\"/\")[-1]\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"{video_id}.md\")\n",
    "    \n",
    "    # Формируем Markdown содержимое\n",
    "    markdown_content = f\"# Транскрипция видео: {video_url}\\n\\n\"\n",
    "    markdown_content += \"## Транскрипция:\\n\\n\"\n",
    "    markdown_content += transcription + \"\\n\\n\"\n",
    "    # markdown_content += \"## Анализ:\\n\\n\"\n",
    "    # markdown_content += analysis + \"\\n\"\n",
    "    \n",
    "    # Сохраняем в файл\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(markdown_content)\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057b588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Обрабатываю видео: https://www.youtube.com/watch?v=s6jJxh72MSc\n",
      "Скачиваю аудио...\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=s6jJxh72MSc\n",
      "[youtube] s6jJxh72MSc: Downloading webpage\n",
      "[youtube] s6jJxh72MSc: Downloading tv client config\n",
      "[youtube] s6jJxh72MSc: Downloading tv player API JSON\n",
      "[youtube] s6jJxh72MSc: Downloading ios player API JSON\n",
      "[youtube] s6jJxh72MSc: Downloading m3u8 information\n",
      "[info] s6jJxh72MSc: Downloading 1 format(s): 251\n",
      "[download] audio/s6jJxh72MSc.m4a has already been downloaded\n",
      "[ExtractAudio] Not converting audio audio/s6jJxh72MSc.m4a; file is already in target format m4a\n",
      "Обрабатываю аудио...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1c/l0_ndlvs3l578q2mzbh87q280000gn/T/ipykernel_9037/1023990989.py:128: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(audio_path, sr=sr)\n",
      "/Users/ivan/develop/venv/lib/python3.13/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "100%|██████████| 146/146 [00:04<00:00, 36.21it/s]\n",
      "/Users/ivan/develop/venv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кластеризуем спикеров\n",
      "Транскрибируем со спикерами\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:00<00:00, 250924.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Формирую транскрибацию\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:00<00:00, 2646890.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохраняю результаты...\n",
      "Результаты сохранены в: transcriptions/s6jJxh72MSc.md\n",
      "\n",
      "Обрабатываю видео: https://youtu.be/s_2W-bQ689w\n",
      "Скачиваю аудио...\n",
      "[youtube] Extracting URL: https://youtu.be/s_2W-bQ689w\n",
      "[youtube] s_2W-bQ689w: Downloading webpage\n",
      "[youtube] s_2W-bQ689w: Downloading tv client config\n",
      "[youtube] s_2W-bQ689w: Downloading tv player API JSON\n",
      "[youtube] s_2W-bQ689w: Downloading ios player API JSON\n",
      "[youtube] s_2W-bQ689w: Downloading m3u8 information\n",
      "[info] s_2W-bQ689w: Downloading 1 format(s): 251\n",
      "[download] audio/s_2W-bQ689w.m4a has already been downloaded\n",
      "[ExtractAudio] Not converting audio audio/s_2W-bQ689w.m4a; file is already in target format m4a\n",
      "Обрабатываю аудио...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1c/l0_ndlvs3l578q2mzbh87q280000gn/T/ipykernel_9037/1023990989.py:128: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(audio_path, sr=sr)\n",
      "/Users/ivan/develop/venv/lib/python3.13/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "100%|██████████| 143/143 [00:04<00:00, 34.98it/s]\n",
      "/Users/ivan/develop/venv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кластеризуем спикеров\n",
      "Транскрибируем со спикерами\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 112447.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Формирую транскрибацию\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 343795.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохраняю результаты...\n",
      "Результаты сохранены в: transcriptions/s_2W-bQ689w.md\n",
      "\n",
      "Обрабатываю видео: https://youtu.be/OiZnJvgUn-M\n",
      "Скачиваю аудио...\n",
      "[youtube] Extracting URL: https://youtu.be/OiZnJvgUn-M\n",
      "[youtube] OiZnJvgUn-M: Downloading webpage\n",
      "[youtube] OiZnJvgUn-M: Downloading tv client config\n",
      "[youtube] OiZnJvgUn-M: Downloading tv player API JSON\n",
      "[youtube] OiZnJvgUn-M: Downloading ios player API JSON\n",
      "[youtube] OiZnJvgUn-M: Downloading m3u8 information\n",
      "[info] OiZnJvgUn-M: Downloading 1 format(s): 140\n",
      "[download] audio/OiZnJvgUn-M.m4a has already been downloaded\n",
      "[download] 100% of    4.57MiB\n",
      "[ExtractAudio] Not converting audio audio/OiZnJvgUn-M.m4a; file is already in target format m4a\n",
      "Обрабатываю аудио...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1c/l0_ndlvs3l578q2mzbh87q280000gn/T/ipykernel_9037/1023990989.py:128: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(audio_path, sr=sr)\n",
      "/Users/ivan/develop/venv/lib/python3.13/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "100%|██████████| 147/147 [00:04<00:00, 36.35it/s]\n",
      "/Users/ivan/develop/venv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кластеризуем спикеров\n",
      "Транскрибируем со спикерами\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 215200.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Формирую транскрибацию\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 1304326.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохраняю результаты...\n",
      "Результаты сохранены в: transcriptions/OiZnJvgUn-M.md\n",
      "\n",
      "Обрабатываю видео: https://youtu.be/jCy9q4OSTww\n",
      "Скачиваю аудио...\n",
      "[youtube] Extracting URL: https://youtu.be/jCy9q4OSTww\n",
      "[youtube] jCy9q4OSTww: Downloading webpage\n",
      "[youtube] jCy9q4OSTww: Downloading tv client config\n",
      "[youtube] jCy9q4OSTww: Downloading tv player API JSON\n",
      "[youtube] jCy9q4OSTww: Downloading ios player API JSON\n",
      "[youtube] jCy9q4OSTww: Downloading m3u8 information\n",
      "[info] jCy9q4OSTww: Downloading 1 format(s): 251\n",
      "[download] audio/jCy9q4OSTww.m4a has already been downloaded\n",
      "[ExtractAudio] Not converting audio audio/jCy9q4OSTww.m4a; file is already in target format m4a\n",
      "Обрабатываю аудио...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1c/l0_ndlvs3l578q2mzbh87q280000gn/T/ipykernel_9037/1023990989.py:128: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(audio_path, sr=sr)\n",
      "/Users/ivan/develop/venv/lib/python3.13/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "100%|██████████| 304/304 [00:08<00:00, 35.03it/s]\n",
      "/Users/ivan/develop/venv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кластеризуем спикеров\n",
      "Транскрибируем со спикерами\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:00<00:00, 180000.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Формирую транскрибацию\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:00<00:00, 2565448.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохраняю результаты...\n",
      "Результаты сохранены в: transcriptions/jCy9q4OSTww.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_of_videos = [\"https://www.youtube.com/watch?v=s6jJxh72MSc\",\n",
    "\"https://youtu.be/s_2W-bQ689w\",\n",
    "\"https://youtu.be/OiZnJvgUn-M\",\n",
    "'https://youtu.be/jCy9q4OSTww']\n",
    "\n",
    "for video_url in list_of_videos:\n",
    "    print(f\"\\nОбрабатываю видео: {video_url}\")\n",
    "    \n",
    "    # 1. Скачиваем аудио\n",
    "    print(\"Скачиваю аудио...\")\n",
    "    audio_path = download_youtube_audio(video_url)\n",
    "    if not audio_path:\n",
    "        print(f\"Не удалось скачать аудио для {video_url}\")\n",
    "        continue\n",
    "    \n",
    "    # 2. Обрабатываем аудио\n",
    "    print(\"Обрабатываю аудио...\")\n",
    "    transcription, analysis = process_audio(audio_path)\n",
    "    # if not transcription or not analysis:\n",
    "    #     print(f\"Не удалось обработать аудио для {video_url}\")\n",
    "    #     continue\n",
    "    \n",
    "    # 3. Сохраняем результаты\n",
    "    print(\"Сохраняю результаты...\")\n",
    "    md_path = save_to_markdown(video_url, transcription, analysis)\n",
    "    print(f\"Результаты сохранены в: {md_path}\")\n",
    "    \n",
    "    # # 4. Удаляем временный аудиофайл (опционально)\n",
    "    # cleanup_files(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d774fe80",
   "metadata": {},
   "source": [
    "### Подгрузка и проверка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfcf6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51866, 1280)\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# подгружаем модель large \n",
    "\n",
    "import whisper\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "def load_whisper_model():\n",
    "    return whisper.load_model(\"large-v3\")\n",
    "\n",
    "load_whisper_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454dadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivan/develop/venv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Славься, Славься, Белорусская Победа, Россия, Россия, В этом слове огонь и сила, В этом слове победы пламя. Поднимаем России знамя. Россия, Россия, В этом слове огонь и сила, В этом слове победы пламя. Поднимаем России знамя. Пусть время нас бурным потоком несёт, За нами Россия, за нами народ. Традиции святы и тысячи лет, Продолжится летопись наших побед. А если врагов налетит вороньё, Их снова Отечество встретит моё. Россия, Россия, В этом слове огонь и сила, В этом слове победы пламя. Поднимаем России знамя. Россия, Россия, В этом слове огонь и сила, В этом слове победы пламя. Поднимаем России знамя. ... В этом слове источник силы, Повторяем вперед Россия! Россия, Россия, В этом слове огонь и сила, В этом слове победы пламя, Поднимаем Россию знамя! Россия, Россия, В этом слове огонь и сила, В этом слове победы пламя, Поднимаем Россию знамя!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверяем модель на песне Газманова \n",
    "\n",
    "def transcribe_audio(model, audio_file):\n",
    "    # Явно указываем язык и задачу\n",
    "    result = model.transcribe(audio_file, language=\"russian\", task=\"transcribe\")\n",
    "    return result[\"text\"]\n",
    "\n",
    "transcribe_audio(model=load_whisper_model(), audio_file=\"gaz.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b6d690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
